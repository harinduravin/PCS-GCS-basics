{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T20:23:39.466045Z",
     "start_time": "2025-10-18T20:23:39.459243Z"
    }
   },
   "cell_type": "code",
   "source": "import ccdm",
   "id": "bf78b4f5b0dcb665",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab7611f5d8405c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 11:08:58.331430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760774938.351371 2940517 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760774938.357654 2940517 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760774938.374497 2940517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760774938.374513 2940517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760774938.374515 2940517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760774938.374517 2940517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-18 11:08:58.379876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded output (1-based symbols): [1, 2, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 3, 4, 3, 2, 2, 3, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 3, 1, 2, 1, 3, 1, 2, 1, 1, 2, 3, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 2, 2, 3, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 3, 2, 4, 2, 2, 1, 1, 3, 2, 1, 2, 1, 3, 2, 1, 1, 2, 2, 2, 1, 2, 3, 1, 2, 1, 1, 1, 2, 1, 1, 3, 4, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 3, 2, 2, 2, 1, 1, 1, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 3, 1, 1, 2, 1, 2, 1, 3, 3, 1, 2, 1, 1, 2, 1, 1, 1, 3, 2, 1, 2, 2, 1, 2, 1, 4, 1, 1, 2, 1, 1, 3, 1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 3, 3, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 4, 2, 3, 1, 2, 1, 1, 1, 3, 2, 1, 1, 3, 1, 1, 2, 3, 1, 1, 4, 2]\n",
      "Encoded output (1-based symbols) tf: [1 2 1 2 1 1 1 3 2 1 1 1 3 4 3 2 2 3 2 2 1 1 2 2 1 1 1 1 1 2 1 1 1 1 2 2 1\n",
      " 2 3 2 1 1 1 1 1 2 1 2 1 3 1 2 1 3 1 2 1 1 2 3 1 1 2 2 1 2 1 1 1 2 1 1 1 3\n",
      " 1 1 2 2 3 1 1 3 2 1 1 1 1 1 2 1 2 1 1 2 1 2 1 3 2 4 2 2 1 1 3 2 1 2 1 3 2\n",
      " 1 1 2 2 2 1 2 3 1 2 1 1 1 2 1 1 3 4 2 1 2 1 1 1 2 1 2 1 1 1 3 2 2 2 1 1 1\n",
      " 1 1 2 3 2 1 1 1 1 1 2 1 2 2 2 2 2 3 1 1 2 1 2 1 3 3 1 2 1 1 2 1 1 1 3 2 1\n",
      " 2 2 1 2 1 4 1 1 2 1 1 3 1 1 1 2 2 1 3 1 1 1 1 1 2 1 2 1 1 2 1 2 1 3 3 2 1\n",
      " 1 1 2 2 1 1 1 2 2 1 1 1 1 4 2 3 1 2 1 1 1 3 2 1 1 3 1 1 2 3 1 1 4 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760774941.510156 2940517 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3244 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 1g.5gb, pci bus id: 0000:03:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "# constant_composition_dm_tf.py\n",
    "import math\n",
    "from typing import List, Sequence\n",
    "import tensorflow as tf\n",
    "import ccdm\n",
    "import numpy as np\n",
    "from ccdm_initialize import initialize\n",
    "\n",
    "# ---------------------------\n",
    "# Helper combinatorics utils\n",
    "# ---------------------------\n",
    "def nchoosek_log2(n: int, k: int) -> float:\n",
    "    \"\"\"log2(n choose k) with safe handling (uses math.comb).\"\"\"\n",
    "    k = min(k, n - k)\n",
    "    if k < 0:\n",
    "        return float('-inf')\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    # math.comb available in Python 3.8+\n",
    "    res = 0.0\n",
    "    # accumulate as in original C++ to avoid huge intermediate factorials\n",
    "    for i in range(1, k + 1):\n",
    "        res += math.log((n - (k - i)) / i, 2.0)\n",
    "    return res\n",
    "\n",
    "def nchooseks_log2(n: int, k_list: Sequence[int]) -> float:\n",
    "    \"\"\"Sum of log2( n choose k_i ), subtracting k_i from n after each.\"\"\"\n",
    "    total = 0.0\n",
    "    n_local = n\n",
    "    for ki in k_list:\n",
    "        total += nchoosek_log2(n_local, ki)\n",
    "        n_local -= ki\n",
    "    return total\n",
    "\n",
    "# ---------------------------\n",
    "# Data classes (simple Python)\n",
    "# ---------------------------\n",
    "class CodeCandidate:\n",
    "    def __init__(self, lower: float, upper: float, prob: float, symbols: List[int]):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.prob = prob\n",
    "        self.symbols = list(symbols)\n",
    "\n",
    "class SourceInterval:\n",
    "    def __init__(self, lower: float = 0.0, upper: float = 1.0):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "class CodeCandidateList:\n",
    "    def __init__(self, k: int = 0):\n",
    "        self.k = k\n",
    "        self.list: List[CodeCandidate] = []\n",
    "\n",
    "# ---------------------------\n",
    "# Core routines\n",
    "# ---------------------------\n",
    "def update_src_interval(src_interval: SourceInterval, src_probability: Sequence[float], src_symbol: int):\n",
    "    \"\"\"Update source interval based on binary source probabilities (1-based symbol).\"\"\"\n",
    "    # src_probability expected length 2 for this coder (binary source)\n",
    "    new_border = src_interval.lower + (src_interval.upper - src_interval.lower) * src_probability[0]\n",
    "    if src_symbol == 0:\n",
    "        src_interval.upper = new_border\n",
    "    else:\n",
    "        src_interval.lower = new_border\n",
    "\n",
    "def update_code_candidates(cc_list: CodeCandidateList, n_i: Sequence[int]):\n",
    "    \"\"\"Rebuild code candidates from remaining composition n_i (0-based counts).\"\"\"\n",
    "    cc_list.list.clear()\n",
    "    k = cc_list.k\n",
    "    n_total = int(sum(n_i))\n",
    "    sum_p = 0.0\n",
    "    if n_total == 0:\n",
    "        return\n",
    "    for i in range(k):\n",
    "        p_i = float(n_i[i]) / float(n_total) if n_total > 0 else 0.0\n",
    "        lower = sum_p\n",
    "        sum_p += p_i\n",
    "        upper = 1.0 if (i == k - 1) else sum_p\n",
    "        candidate = CodeCandidate(lower=lower, upper=upper, prob=p_i, symbols=[i + 1])\n",
    "        cc_list.list.append(candidate)\n",
    "\n",
    "def find_identified_code_candidate_index(src_interval: SourceInterval, cc_list: CodeCandidateList, n_i: Sequence[int]) -> int:\n",
    "    \"\"\"Return index of a code candidate whose lower bound lies inside source interval and count > 0.\"\"\"\n",
    "    for idx, cc in enumerate(cc_list.list):\n",
    "        if src_interval.lower <= cc.lower < src_interval.upper and n_i[idx] != 0:\n",
    "            return idx\n",
    "    return -1\n",
    "\n",
    "def update_N_i(n_i: List[int], symbol_list: Sequence[int]):\n",
    "    \"\"\"Decrease counts for each symbol in symbol_list (symbols are 1-based).\"\"\"\n",
    "    for sym in symbol_list:\n",
    "        n_i[sym - 1] -= 1\n",
    "\n",
    "def find_code_interval_from_candidates(cc_list: CodeCandidateList, search_list: List[int]) -> SourceInterval:\n",
    "    \"\"\"Return interval for exact candidate whose symbols equal search_list.\"\"\"\n",
    "    for cc in cc_list.list:\n",
    "        if cc.symbols == search_list:\n",
    "            return SourceInterval(lower=cc.lower, upper=cc.upper)\n",
    "    # default empty interval (shouldn't usually happen)\n",
    "    return SourceInterval(0.0, 0.0)\n",
    "\n",
    "def finalize_code_symbols(src_interval: SourceInterval, cc_list: CodeCandidateList, n_i: List[int]) -> List[int]:\n",
    "    \"\"\"If an identified single code candidate exists, finalize symbols (append remaining counts).\"\"\"\n",
    "    cc_index = find_identified_code_candidate_index(src_interval, cc_list, n_i)\n",
    "    if cc_index == -1:\n",
    "        return []\n",
    "    cc = cc_list.list[cc_index]\n",
    "    symbols_new = list(cc.symbols)  # copy\n",
    "    update_N_i(n_i, cc.symbols)\n",
    "    # append remaining counts in order 1..k\n",
    "    for i in range(cc_list.k):\n",
    "        for _ in range(n_i[i]):\n",
    "            symbols_new.append(i + 1)\n",
    "    return symbols_new\n",
    "\n",
    "def check_for_output_and_rescale(src_interval: SourceInterval, cc_list: CodeCandidateList, n_i: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Check whether src_interval is fully contained in any code candidate interval.\n",
    "    If yes, rescale the source interval wrt that candidate, update counts and candidates,\n",
    "    and repeat until no such containing candidate exists.\n",
    "    Returns the concatenated list of produced code symbols during the process.\n",
    "    \"\"\"\n",
    "    produced_symbols: List[int] = []\n",
    "\n",
    "    # find initial candidate that contains the entire source interval\n",
    "    chosen_cc = None\n",
    "    for cc in cc_list.list:\n",
    "        if src_interval.lower >= cc.lower and src_interval.upper <= cc.upper:\n",
    "            chosen_cc = cc\n",
    "            break\n",
    "\n",
    "    while chosen_cc is not None:\n",
    "        # rescale source interval to candidate's sub-interval\n",
    "        interval_width = (chosen_cc.upper - chosen_cc.lower)\n",
    "        if interval_width <= 0:\n",
    "            # numerical safeties\n",
    "            break\n",
    "        src_interval.lower = (src_interval.lower - chosen_cc.lower) / interval_width\n",
    "        src_interval.upper = (src_interval.upper - chosen_cc.lower) / interval_width\n",
    "        # clamp tiny overshoots\n",
    "        if src_interval.upper > 1.0:\n",
    "            src_interval.upper = 1.0\n",
    "\n",
    "        # consume candidate symbols\n",
    "        produced_symbols.extend(chosen_cc.symbols)\n",
    "        update_N_i(n_i, chosen_cc.symbols)\n",
    "\n",
    "        # rebuild candidate list with new counts\n",
    "        update_code_candidates(cc_list, n_i)\n",
    "\n",
    "        # find next chosen candidate\n",
    "        chosen_cc = None\n",
    "        for cc in cc_list.list:\n",
    "            if src_interval.lower >= cc.lower and src_interval.upper <= cc.upper:\n",
    "                chosen_cc = cc\n",
    "                break\n",
    "\n",
    "    return produced_symbols\n",
    "\n",
    "# ---------------------------\n",
    "# Main encoder function\n",
    "# ---------------------------\n",
    "def encode_constant_composition_arithmetic_matcher(src_symbols: Sequence[int],\n",
    "                                                   n_total: int,\n",
    "                                                   n_i_vect: Sequence[int],\n",
    "                                                   src_prob: Sequence[float] = (0.5, 0.5)) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      src_symbols: sequence/iterable of source symbols (each either 0 or 1 -- 1-based used in original C++: symbol==0 or 1)\n",
    "                   In the original C++ psrc = {0.5, 0.5} and src_symbol is 0/1.\n",
    "      n_total: total number of output symbols (n)\n",
    "      n_i_vect: list of k integers specifying composition counts (length k); sum(n_i_vect) must equal n_total\n",
    "      src_prob: binary source probabilities, default .5/.5\n",
    "    Returns:\n",
    "      Tensor of output code symbols (dtype tf.int32), 1-based symbol indices as in C++ implementation.\n",
    "    \"\"\"\n",
    "    # convert inputs to Python lists for control flow\n",
    "    k = len(n_i_vect)\n",
    "    n_i = list(n_i_vect)  # mutable copy\n",
    "\n",
    "    # initialize source interval and code-candidate list\n",
    "    src_interval = SourceInterval(0.0, 1.0)\n",
    "    cc_list = CodeCandidateList(k=k)\n",
    "    update_code_candidates(cc_list, n_i)\n",
    "\n",
    "    code_symbols: List[int] = []\n",
    "\n",
    "    # m (bits) computed in original but unused for core symbol generation; we keep it for parity if desired\n",
    "    m_bits = nchooseks_log2(n_total, list(n_i_vect))\n",
    "\n",
    "    # loop through source symbols, update interval, and try to output symbols\n",
    "    for s in src_symbols:\n",
    "        # s is expected to be 0 or 1. If user gives 1-based, convert to 1-based? original expects 0 or 1.\n",
    "        # The C++ call used psrc[2] = {0.5, 0.5}; updateSrcInterval(..., src_symbol)\n",
    "        # where src_symbol is *it from src_symbols, so they were 0 or 1.\n",
    "        src_sym = int(s)\n",
    "        update_src_interval(src_interval, src_prob, src_sym)\n",
    "        new_syms = check_for_output_and_rescale(src_interval, cc_list, n_i)\n",
    "        if new_syms:\n",
    "            code_symbols.extend(new_syms)\n",
    "\n",
    "    # finalize\n",
    "    fin_syms = finalize_code_symbols(src_interval, cc_list, n_i)\n",
    "    code_symbols.extend(fin_syms)\n",
    "\n",
    "    return tf.convert_to_tensor(code_symbols, dtype=tf.int32)\n",
    "\n",
    "# ---------------------------\n",
    "# Example / quick test\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # small example: composition of 3 symbols across k=3 symbols: counts [1,1,1] (n=3)\n",
    "    # use a short pseudo source stream (binary symbols 0/1)\n",
    "    # src = [0, 1, 0, 1, 1, 0]  # arbitrary\n",
    "    # n_total = 3\n",
    "    # n_i_vect = [1, 1, 1]  # three output symbols, each one of symbol indices {1,2,3}\n",
    "\n",
    "    # out = encode_constant_composition_arithmetic_matcher(src, n_total, n_i_vect)\n",
    "    # print(\"Encoded output (1-based symbols):\", out.numpy())\n",
    "\n",
    "    # small example: C++ implementation\n",
    "    # use a short pseudo source stream (binary symbols 0/1)\n",
    "    pOpt = np.array([0.537577302140556,0.322026673986155,0.115556317270981,0.024839706602308])\n",
    "    n = 256\n",
    "    [p_quant,num_info_bits,n_i] = initialize(pOpt,256)\n",
    "    src_symbols = np.random.randint(0, 2, size=num_info_bits)\n",
    "    code_symbols = ccdm.encode(src_symbols, n, n_i)\n",
    "\n",
    "    # out = encode_constant_composition_arithmetic_matcher(src, n_total, n_i_vect)\n",
    "    print(\"Encoded output (1-based symbols):\", code_symbols)\n",
    "\n",
    "    out = encode_constant_composition_arithmetic_matcher(src_symbols, n, n_i)\n",
    "    print(\"Encoded output (1-based symbols) tf:\", out.numpy())\n"
   ],
   "id": "7f72da3a14a29c70"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a527f86-3def-473d-8063-6d005429f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def batch_ccdm_encode(src_symbols,        # shape [B, T], dtype int32, values 0 or 1\n",
    "                      n_total,            # scalar int (total output length n)\n",
    "                      n_i_vect,           # shape [k], dtype int32, composition counts\n",
    "                      src_prob=(0.5, 0.5) # binary source probabilities (python tuple ok)\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Batch CCDM encoder (tensor only).\n",
    "    Returns: tf.Tensor shape [B, n_total], dtype tf.int32, 1-based symbol indices.\n",
    "    Raises: tf.errors.InvalidArgumentError if final lengths != n_total or no finalize candidate found.\n",
    "    \"\"\"\n",
    "\n",
    "    src_prob = tf.convert_to_tensor(src_prob, dtype=tf.float64)  # length 2\n",
    "    src_prob0 = tf.cast(src_prob[0], tf.float64)\n",
    "\n",
    "    src_symbols = tf.convert_to_tensor(src_symbols, dtype=tf.int32)\n",
    "    n_i_vect = tf.convert_to_tensor(n_i_vect, dtype=tf.int32)\n",
    "    n_total = tf.convert_to_tensor(n_total, dtype=tf.int32)\n",
    "\n",
    "    B = tf.shape(src_symbols)[0]\n",
    "    T = tf.shape(src_symbols)[1]\n",
    "    k = tf.shape(n_i_vect)[0]\n",
    "\n",
    "    # initialize per-batch mutable state\n",
    "    # n_i per batch: shape [B, k], init with broadcast of n_i_vect\n",
    "    n_i = tf.cast(tf.broadcast_to(n_i_vect[tf.newaxis, :], [B, k]), tf.int32)\n",
    "\n",
    "    # compute initial candidate bounds from n_i\n",
    "    def compute_bounds_from_n_i(n_i_batch):\n",
    "        # n_i_batch shape [k], dtype int32\n",
    "        n_total_batch = tf.cast(tf.reduce_sum(n_i_batch), tf.float64)\n",
    "        n_i_f = tf.cast(n_i_batch, tf.float64)\n",
    "        # avoid division by zero (shouldn't happen until finalize)\n",
    "        p = tf.where(n_total_batch > 0.0, n_i_f / n_total_batch, tf.zeros_like(n_i_f))\n",
    "        lower = tf.cumsum(p, exclusive=True)\n",
    "        upper = tf.cumsum(p)\n",
    "        # ensure last upper = 1.0 if any\n",
    "        upper = tf.where(tf.range(k, dtype=tf.int32) == (k - 1), tf.ones_like(upper), upper)\n",
    "        return lower, upper\n",
    "\n",
    "    # vectorized bounds\n",
    "    lower_bounds, upper_bounds = tf.map_fn(lambda r: compute_bounds_from_n_i(r),\n",
    "                                           n_i,\n",
    "                                           dtype=(tf.float64, tf.float64))\n",
    "\n",
    "    # source intervals per batch\n",
    "    src_lower = tf.zeros([B], tf.float64)\n",
    "    src_upper = tf.ones([B], tf.float64)\n",
    "\n",
    "    # output buffer: shape [B, n_total], fill with zeros initially\n",
    "    output = tf.zeros([B, n_total], dtype=tf.int32)\n",
    "    # next write position per batch\n",
    "    write_ptr = tf.zeros([B], dtype=tf.int32)\n",
    "\n",
    "    # helper to find chosen candidate index per batch: first idx with lower <= src_lower < upper and n_i>0\n",
    "    def choose_candidate_indices(src_lower, src_upper, lower_bounds, upper_bounds, n_i):\n",
    "        # masks shape [B, k]\n",
    "        # condition: src_lower >= lower_bounds && src_upper <= upper_bounds AND n_i > 0\n",
    "        cond1 = tf.greater_equal(tf.expand_dims(src_lower, 1), lower_bounds)  # [B,k]\n",
    "        cond2 = tf.less_equal(tf.expand_dims(src_upper, 1), upper_bounds)     # [B,k]\n",
    "        cond3 = tf.greater(n_i, 0)\n",
    "        mask = tf.logical_and(tf.logical_and(cond1, cond2), cond3)\n",
    "        # For each batch, compute smallest index where mask is True. If none True, return -1.\n",
    "        large = tf.cast(k, tf.int32)\n",
    "        idxs = tf.where(mask, tf.tile(tf.range(k)[tf.newaxis, :], [B, 1]), large)\n",
    "        min_idx = tf.reduce_min(idxs, axis=1)  # [B]\n",
    "        chosen_idx = tf.where(tf.equal(min_idx, large), tf.fill([B], -1), min_idx)\n",
    "        return chosen_idx, mask\n",
    "\n",
    "    # inner loop that checks for output and rescales repeatedly until none batch has a containing candidate\n",
    "    # We'll implement it as a tf.while_loop where each iteration processes all batches which currently have a candidate.\n",
    "\n",
    "    # We'll create the outer time-loop below. First, create a step function that:\n",
    "    # - updates src interval from one source symbol (vectorized)\n",
    "    # - then runs the inner \"rescale loop\" (vectorized) to append produced symbols\n",
    "\n",
    "    # tf.range arrays used many times\n",
    "    range_k = tf.range(k, dtype=tf.int32)\n",
    "    range_k_f64 = tf.cast(range_k, tf.float64)\n",
    "\n",
    "    # Build while body for the inner \"while a candidate exists\" loop\n",
    "    def inner_rescale_loop(body_state):\n",
    "        # state: (src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr)\n",
    "        src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr = body_state\n",
    "\n",
    "        chosen_idx, mask = choose_candidate_indices(src_lower, src_upper, lower_bounds, upper_bounds, n_i)\n",
    "        # boolean whether any batch has candidate\n",
    "        any_chosen = tf.reduce_any(tf.greater_equal(chosen_idx, 0))\n",
    "\n",
    "        def some_chosen_branch():\n",
    "            # For batches with chosen_idx >=0 apply vectorized updates.\n",
    "            chosen_present = tf.greater_equal(chosen_idx, 0)  # [B] bool\n",
    "\n",
    "            # gather candidate lower/upper for chosen idx. For batches with idx=-1, we gather zeros and then mask them.\n",
    "            # prepare indices for gather_nd\n",
    "            batch_idxs = tf.reshape(tf.range(B, dtype=tf.int32), [B, 1])  # [B,1]\n",
    "            gather_idx = tf.concat([batch_idxs, tf.reshape(tf.maximum(chosen_idx, 0), [B,1])], axis=1)  # [B,2]\n",
    "            chosen_lower = tf.gather_nd(lower_bounds, gather_idx)  # [B]\n",
    "            chosen_upper = tf.gather_nd(upper_bounds, gather_idx)  # [B]\n",
    "\n",
    "            # only use chosen_lower/upper for batches where chosen_present\n",
    "            chosen_lower = tf.where(chosen_present, chosen_lower, tf.zeros_like(chosen_lower))\n",
    "            chosen_upper = tf.where(chosen_present, chosen_upper, tf.ones_like(chosen_upper))\n",
    "\n",
    "            # compute interval width (avoid zero)\n",
    "            width = chosen_upper - chosen_lower\n",
    "            width_safe = tf.where(width <= 0.0, tf.ones_like(width), width)\n",
    "\n",
    "            # rescale src interval for chosen batches\n",
    "            new_src_lower = (src_lower - chosen_lower) / width_safe\n",
    "            new_src_upper = (src_upper - chosen_lower) / width_safe\n",
    "            new_src_upper = tf.where(new_src_upper > 1.0, tf.ones_like(new_src_upper), new_src_upper)\n",
    "\n",
    "            # update n_i: decrement count for chosen_idx where present\n",
    "            # Build one-hot for chosen_idx: shape [B, k], values 1 at chosen idx, 0 elsewhere. For idx=-1, zero vector.\n",
    "            chosen_idx_pos = tf.maximum(chosen_idx, 0)\n",
    "            one_hot = tf.one_hot(chosen_idx_pos, depth=k, dtype=tf.int32)  # [B,k]\n",
    "            one_hot = one_hot * tf.cast(tf.expand_dims(tf.cast(chosen_present, tf.int32), 1), tf.int32)\n",
    "\n",
    "            n_i_new = n_i - one_hot  # shape [B,k]\n",
    "\n",
    "            # write produced symbol (chosen_idx+1) into output at write_ptr for batches where chosen_present\n",
    "            produced_symbol = (chosen_idx + 1)  # -1 -> 0 but we mask\n",
    "            produced_symbol = tf.where(chosen_present, produced_symbol, tf.zeros_like(produced_symbol))\n",
    "\n",
    "            # For writing, create mask for each batch to do single-element write:\n",
    "            # output shape [B, n_total], we update column at index write_ptr[b] for each batch b that has chosen_present\n",
    "            # Build indices for scatter_nd_update\n",
    "            write_positions = tf.where(chosen_present, write_ptr, tf.zeros_like(write_ptr))  # use 0 for others\n",
    "            scatter_idx = tf.stack([tf.range(B, dtype=tf.int32), write_positions], axis=1)  # [B,2]\n",
    "            # But we should only update for chosen_present. So form updates where chosen_present.\n",
    "            updates = tf.where(chosen_present, produced_symbol, tf.zeros_like(produced_symbol))\n",
    "            output = tf.tensor_scatter_nd_update(output, scatter_idx, updates)\n",
    "\n",
    "            # increment write_ptr only for chosen_present\n",
    "            write_ptr = write_ptr + tf.cast(chosen_present, tf.int32)\n",
    "\n",
    "            # recompute candidate bounds from n_i_new\n",
    "            def compute_bounds_for_all(n_i_full):\n",
    "                # n_i_full shape [B,k] int32 -> returns lower_bounds, upper_bounds [B,k] float64\n",
    "                n_total_batch = tf.cast(tf.reduce_sum(n_i_full, axis=1, keepdims=True), tf.float64)  # [B,1]\n",
    "                n_i_f = tf.cast(n_i_full, tf.float64)  # [B,k]\n",
    "                p = tf.where(n_total_batch > 0.0, n_i_f / n_total_batch, tf.zeros_like(n_i_f))\n",
    "                lower = tf.cumsum(p, axis=1, exclusive=True)\n",
    "                upper = tf.cumsum(p, axis=1)\n",
    "                # ensure last upper = 1.0\n",
    "                last_mask = tf.equal(tf.range(k, dtype=tf.int32), k-1)\n",
    "                last_mask = tf.cast(last_mask, tf.float64)[tf.newaxis, :]\n",
    "                upper = tf.where(tf.equal(last_mask, 1.0), tf.ones_like(upper), upper)\n",
    "                return lower, upper\n",
    "\n",
    "            lower_bounds_new, upper_bounds_new = compute_bounds_for_all(n_i_new)\n",
    "\n",
    "            # For batches that didn't have chosen candidate, keep src_lower, src_upper unchanged and n_i unchanged\n",
    "            src_lower = tf.where(chosen_present, new_src_lower, src_lower)\n",
    "            src_upper = tf.where(chosen_present, new_src_upper, src_upper)\n",
    "            n_i = tf.where(tf.expand_dims(chosen_present, 1), n_i_new, n_i)\n",
    "            lower_bounds = tf.where(tf.expand_dims(chosen_present, 1), lower_bounds_new, lower_bounds)\n",
    "            upper_bounds = tf.where(tf.expand_dims(chosen_present, 1), upper_bounds_new, upper_bounds)\n",
    "\n",
    "            return (src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr)\n",
    "        # no chosen candidate\n",
    "        return (src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr)\n",
    "\n",
    "    # Now define the outer time loop body\n",
    "    # state for outer loop: t, src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr\n",
    "    t0 = tf.constant(0, tf.int32)\n",
    "    state0 = (t0, src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr)\n",
    "\n",
    "    def outer_cond(state):\n",
    "        t, *_ = state\n",
    "        return tf.less(t, T)\n",
    "\n",
    "    def outer_body(state):\n",
    "        t, src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr = state\n",
    "        # symbol at time t for all batches\n",
    "        sym_t = tf.cast(src_symbols[:, t], tf.int32)  # [B], 0 or 1\n",
    "\n",
    "        # Update source interval per batch based on binary probability\n",
    "        # new_border = lower + (upper-lower) * psrc[0]\n",
    "        new_border = src_lower + (src_upper - src_lower) * src_prob0\n",
    "        # If src_symbol == 0 => upper = new_border, else lower = new_border\n",
    "        is_zero = tf.equal(sym_t, 0)\n",
    "        src_upper = tf.where(is_zero, new_border, src_upper)\n",
    "        src_lower = tf.where(is_zero, src_lower, new_border)\n",
    "\n",
    "        # After updating interval, run inner rescale loop until no chosen candidate exists\n",
    "        inner_state = (src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr)\n",
    "\n",
    "        def inner_cond(inner_state):\n",
    "            src_lower_i, src_upper_i, lb_i, ub_i, n_i_i, out_i, wp_i = inner_state\n",
    "            chosen_idx_i, _ = choose_candidate_indices(src_lower_i, src_upper_i, lb_i, ub_i, n_i_i)\n",
    "            return tf.reduce_any(tf.greater_equal(chosen_idx_i, 0))\n",
    "\n",
    "        def inner_body(inner_state):\n",
    "            return inner_rescale_loop(inner_state)\n",
    "\n",
    "        src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr = tf.while_loop(\n",
    "            inner_cond, inner_body, inner_state,\n",
    "            maximum_iterations= n_total * 2  # safe upper bound\n",
    "        )\n",
    "\n",
    "        # advance time\n",
    "        t = t + 1\n",
    "        return (t, src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr)\n",
    "\n",
    "    print(tf.while_loop(outer_cond, outer_body, state0, maximum_iterations=T))\n",
    "    # _, src_lower, src_upper, lower_bounds, upper_bounds, n_i, output, write_ptr = tf.while_loop(outer_cond, outer_body, state0, maximum_iterations=T)\n",
    "    # print(final_state)\n",
    "\n",
    "    # Unpack final state after processing all source symbols\n",
    "     # = final_state\n",
    "\n",
    "    # FINALIZE step for each batch: find identified candidate index (if any), then produce:\n",
    "    # resultSymbols = cc.symbols (i.e., chosen_idx+1) and then append remaining counts of each symbol\n",
    "    chosen_idx, mask = choose_candidate_indices(src_lower, src_upper, lower_bounds, upper_bounds, n_i)\n",
    "    # chosen_idx gives candidate with lower in interval? The C++ used findIdentifiedCodeCandidateIndex which checks:\n",
    "    # if src_interval.lowerBound <= cc.lowerBound  && cc.lowerBound < src_interval.upperBound && n_i[i] != 0\n",
    "    # Our choose_candidate_indices uses lower<=src_lower and src_upper <= upper. For finalization we need the other condition.\n",
    "    # So compute identified candidate differently:\n",
    "    # cond_identified: src_lower <= lower_bounds < src_upper and n_i>0\n",
    "    cond_identified = tf.logical_and(tf.greater_equal(tf.expand_dims(src_lower,1), tf.zeros_like(lower_bounds)),\n",
    "                                     tf.zeros_like(lower_bounds) + 0)  # dummy to later build\n",
    "    # Build mask for identified\n",
    "    cond_low_ge = tf.less_equal(lower_bounds, tf.expand_dims(src_upper,1))  # lower <= src_upper (not exact)\n",
    "    # Let's compute explicitly:\n",
    "    identified_mask = tf.logical_and(\n",
    "        tf.greater_equal(tf.expand_dims(src_upper,1), lower_bounds + 0.0),  # src_upper >= cc.lower\n",
    "        tf.greater_equal(lower_bounds + 0.0, tf.expand_dims(src_lower,1))   # cc.lower >= src_lower\n",
    "    )\n",
    "    identified_mask = tf.logical_and(identified_mask, tf.greater(n_i, 0))\n",
    "    # find smallest index per batch\n",
    "    large = tf.cast(k, tf.int32)\n",
    "    idxs_id = tf.where(identified_mask, tf.tile(tf.range(k)[tf.newaxis,:], [B,1]), large)\n",
    "    min_idx_id = tf.reduce_min(idxs_id, axis=1)\n",
    "    chosen_final_idx = tf.where(tf.equal(min_idx_id, large), tf.fill([B], -1), min_idx_id)\n",
    "\n",
    "    # For robustness: if any chosen_final_idx == -1 -> error\n",
    "    any_missing = tf.reduce_any(tf.equal(chosen_final_idx, -1))\n",
    "    def raise_error():\n",
    "        # throw TF error\n",
    "        msg = \"CCDM finalize: no identified code candidate for at least one batch (cannot finalize).\"\n",
    "        return tf.debugging.assert_less_equal(1, 0, message=msg)  # intentional assert fail\n",
    "    def continue_finalize():\n",
    "        return tf.no_op()\n",
    "\n",
    "    tf.cond(any_missing, lambda: raise_error(), lambda: continue_finalize())\n",
    "\n",
    "    # Now write final symbols per batch: first the chosen_final_idx + 1 (the cc.symbols)\n",
    "    produced_symbol_final = chosen_final_idx + 1  # shape [B]\n",
    "\n",
    "    # scatter into output at write_ptr if chosen_final present (it is)\n",
    "    scatter_idx = tf.stack([tf.range(B, dtype=tf.int32), write_ptr], axis=1)\n",
    "    output = tf.tensor_scatter_nd_update(output, scatter_idx, produced_symbol_final)\n",
    "    write_ptr = write_ptr + 1\n",
    "\n",
    "    # Then append remaining counts for each symbol i (1..k)\n",
    "    # For each symbol i, repeat symbol i+1 n_i[b,i] times for each batch b, writing sequentially at write_ptr\n",
    "    # We'll perform this by iterating i from 0..k-1 using a small tf.while_loop (k is typically modest).\n",
    "    i0 = tf.constant(0, dtype=tf.int32)\n",
    "    cond_k = lambda i, *_: tf.less(i, k)\n",
    "\n",
    "    def body_k(i, output, write_ptr, n_i):\n",
    "        # per-batch count to append\n",
    "        counts = n_i[:, i]  # [B]\n",
    "        sym_val = tf.fill([B], i + 1)  # [B], 1-based\n",
    "        # For each batch, we need to write counts[b] copies of sym_val[b] at increasing write_ptr.\n",
    "        # We'll loop per repetition across all batches using another loop up to max_count (max across batches).\n",
    "        max_count = tf.reduce_max(counts)\n",
    "\n",
    "        j0 = tf.constant(0, dtype=tf.int32)\n",
    "        def cond_j(j, output, write_ptr, counts):\n",
    "            return tf.less(j, max_count)\n",
    "        def body_j(j, output, write_ptr, counts):\n",
    "            # mask for batches where counts > j\n",
    "            mask = tf.greater(counts, j)  # [B]\n",
    "            write_positions = tf.where(mask, write_ptr, tf.zeros_like(write_ptr))\n",
    "            scatter_idx = tf.stack([tf.range(B, dtype=tf.int32), write_positions], axis=1)\n",
    "            updates = tf.where(mask, sym_val, tf.zeros_like(sym_val))\n",
    "            output = tf.tensor_scatter_nd_update(output, scatter_idx, updates)\n",
    "            write_ptr = write_ptr + tf.cast(mask, tf.int32)\n",
    "            return j + 1, output, write_ptr, counts\n",
    "        _, output, write_ptr, _ = tf.while_loop(cond_j, body_j, (j0, output, write_ptr, counts), maximum_iterations=n_total)\n",
    "        # set counts to zero for this symbol now\n",
    "        n_i = tf.tensor_scatter_nd_update(n_i, tf.reshape(tf.range(B, dtype=tf.int32), [-1,1]), n_i)  # no-op to keep shape\n",
    "        return i + 1, output, write_ptr, n_i\n",
    "\n",
    "    _, output, write_ptr, n_i = tf.while_loop(cond_k, body_k, (i0, output, write_ptr, n_i),\n",
    "                                             maximum_iterations=k)\n",
    "\n",
    "    # Final check: write_ptr must equal n_total for all batches\n",
    "    all_done = tf.reduce_all(tf.equal(write_ptr, n_total))\n",
    "    tf.debugging.assert_equal(all_done, True, message=\"CCDM failed: produced symbol count != n_total for some batch.\")\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31b27653-02da-4561-817b-e8cd6c15c0a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/tmp/rjayarat/5282792/ipykernel_2940517/856147342.py\", line 209, in batch_ccdm_encode  *\n        print(tf.while_loop(outer_cond, outer_body, state0, maximum_iterations=T))\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__batch_ccdm_encode.<locals>.outer_cond() takes 1 positional argument but 8 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m n_i \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(n_i, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint32)\n\u001B[1;32m      5\u001B[0m src_symbols \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, size\u001B[38;5;241m=\u001B[39mnum_info_bits, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint32),(\u001B[38;5;241m1\u001B[39m,num_info_bits))\n\u001B[0;32m----> 6\u001B[0m code_symbols \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_ccdm_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_symbols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_i\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# def batch_ccdm_encode(src_symbols,        # shape [B, T], dtype int32, values 0 or 1\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m#                       n_total,            # scalar int (total output length n)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#                       n_i_vect,           # shape [k], dtype int32, composition counts\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m#                       src_prob=(0.5, 0.5) # binary source probabilities (python tuple ok)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#                      ):\u001B[39;00m\n",
      "File \u001B[0;32m/projappl/project_2013933/Sionna_New/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/tmp/rjayarat/5282792/__autograph_generated_fileneklpp6_.py:216\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__batch_ccdm_encode\u001B[0;34m(src_symbols, n_total, n_i_vect, src_prob)\u001B[0m\n\u001B[1;32m    214\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    215\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fscope_7\u001B[38;5;241m.\u001B[39mret(retval__7, do_return_7)\n\u001B[0;32m--> 216\u001B[0m ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mprint\u001B[39m)(\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhile_loop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mouter_cond\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mouter_body\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate0\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmaximum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    217\u001B[0m (chosen_idx, mask) \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(choose_candidate_indices), (ag__\u001B[38;5;241m.\u001B[39mld(src_lower), ag__\u001B[38;5;241m.\u001B[39mld(src_upper), ag__\u001B[38;5;241m.\u001B[39mld(lower_bounds), ag__\u001B[38;5;241m.\u001B[39mld(upper_bounds), ag__\u001B[38;5;241m.\u001B[39mld(n_i)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m    218\u001B[0m cond_identified \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mlogical_and, (ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mgreater_equal, (ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mexpand_dims, (ag__\u001B[38;5;241m.\u001B[39mld(src_lower), \u001B[38;5;241m1\u001B[39m), \u001B[38;5;28;01mNone\u001B[39;00m, fscope), ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mzeros_like, (ag__\u001B[38;5;241m.\u001B[39mld(lower_bounds),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope), ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mzeros_like, (ag__\u001B[38;5;241m.\u001B[39mld(lower_bounds),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0\u001B[39m), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n",
      "\u001B[0;31mTypeError\u001B[0m: in user code:\n\n    File \"/tmp/rjayarat/5282792/ipykernel_2940517/856147342.py\", line 209, in batch_ccdm_encode  *\n        print(tf.while_loop(outer_cond, outer_body, state0, maximum_iterations=T))\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__batch_ccdm_encode.<locals>.outer_cond() takes 1 positional argument but 8 were given\n"
     ]
    }
   ],
   "source": [
    "pOpt = np.array([0.537577302140556,0.322026673986155,0.115556317270981,0.024839706602308])\n",
    "n = 256\n",
    "[p_quant,num_info_bits,n_i] = initialize(pOpt,256)\n",
    "n_i = np.array(n_i, dtype=np.int32)\n",
    "src_symbols = np.reshape(np.random.randint(0, 2, size=num_info_bits, dtype=np.int32),(1,num_info_bits))\n",
    "code_symbols = batch_ccdm_encode(src_symbols, n, n_i)\n",
    "# def batch_ccdm_encode(src_symbols,        # shape [B, T], dtype int32, values 0 or 1\n",
    "#                       n_total,            # scalar int (total output length n)\n",
    "#                       n_i_vect,           # shape [k], dtype int32, composition counts\n",
    "#                       src_prob=(0.5, 0.5) # binary source probabilities (python tuple ok)\n",
    "#                      ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf360f33-94e7-4f1f-8b30-da4bf5c12664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed0a77-a25e-4bcb-8704-f5c48c59aff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
